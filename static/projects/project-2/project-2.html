<div style="font-family: Arial, sans-serif; line-height: 1.6; color: #333; padding: 20px; background-color: #f9f9f9; border-radius: 10px; box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); text-align: justify; max-width: 800px; margin: 20px auto;">
    <h2 style="color: #333; font-size: 24px; margin-top: 20px; border-bottom: 2px solid #ddd; padding-bottom: 5px; text-align: left;">Project Description</h2>

    <h3 style="color: #333; font-size: 20px; margin-top: 15px;">Situation:</h3>
    <p style="margin-left: 20px;">In a recent project, I was tasked with improving the performance of a machine learning model that was suffering from a relatively high log loss. The model was crucial for predicting a binary outcome in a medical context, and its predictive accuracy needed enhancement.</p>

    <h3 style="color: #333; font-size: 20px; margin-top: 15px;">Task:</h3>
    <p style="margin-left: 20px;">The specific task was to identify and implement strategies to decrease the log loss of the machine learning model. This required a comprehensive assessment of the model's performance, feature engineering, hyper-parameter tuning, and exploring different algorithms to optimize its predictive capabilities.</p>

    <h3 style="color: #333; font-size: 20px; margin-top: 15px;">Action:</h3>
    <ul style="margin-left: 40px; list-style-type: disc; padding-left: 20px;">
        <li><strong>Feature Engineering:</strong> Conducted a thorough analysis of existing features and introduced new ones that were more informative for the model.</li>
        <li><strong>Hyper-parameter Tuning:</strong> Employed grid search and cross-validation to find the optimal hyper-parameters, focusing on regularization, learning rates, and tree depth.</li>
        <li><strong>Model Selection:</strong> Experimented with various machine learning algorithms, including Random Forests, Gradient Boosting, and XGBoost, to identify the most suitable model for the problem.</li>
        <li><strong>Ensemble Methods:</strong> Created an ensemble of models using stacking to leverage the strengths of multiple algorithms.</li>
    </ul>
    <p style="margin-left: 20px;">Here is a code snippet showcasing the ensemble model:</p>
    <pre style="background-color: #f0f0f0; padding: 15px; border-radius: 5px; overflow: auto; margin-left: 20px; font-size: 14px; line-height: 1.5;"><code class="language-Python">rf_model = RandomForestClassifier(random_state=42)
gb_model = GradientBoostingClassifier(random_state=42)

# Create an ensemble model using a voting classifier
ensemble_model = VotingClassifier(estimators=[
    ('RandomForest', rf_model),
    ('GradientBoosting', gb_model)
], voting='soft')  # 'soft' enables predicted probabilities for log_loss

# Create the full pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                             ('ensemble', ensemble_model)])</code></pre>

    <h3 style="color: #333; font-size: 20px; margin-top: 15px;">Result:</h3>
    <p style="margin-left: 20px;">The collective efforts led to a significant reduction in log loss, indicating a substantial improvement in the model's predictive performance. The optimized model demonstrated enhanced generalization on both the training and test datasets.</p>
    <img src="/static/projects/project-2/cirr_sub.png" alt="Web Application Snapshot" style="display: block; margin: 20px auto; max-width: 100%; border-radius: 5px;"/>

    <h3 style="color: #333; font-size: 20px; margin-top: 15px;">Skills and Technologies Implemented:</h3>
    <ul style="margin-left: 40px; list-style-type: disc; padding-left: 20px;">
        <li>Machine Learning (Random Forests, Gradient Boosting, XGBoost)</li>
        <li>Feature Engineering</li>
        <li>Hyper-parameter Tuning</li>
        <li>Ensemble Methods</li>
        <li>Python Programming</li>
        <li>Pandas, Numpy</li>
        <li>Matplotlib, Scikit-learn</li>
    </ul>

    <h3 style="color: #333; font-size: 20px; margin-top: 15px;">Reflection:</h3>
    <p style="margin-left: 20px;">This experience highlighted the importance of a systematic approach to model improvement. It emphasized the impact of feature engineering, hyperparameter tuning, and the selection of appropriate algorithms in achieving better predictive outcomes. Additionally, addressing class imbalance and using ensemble methods contributed to the overall success of the model. Going forward, I intend to apply similar strategies to future projects, continually refining and adapting my approach based on the unique characteristics of each problem.</p>
</div>
